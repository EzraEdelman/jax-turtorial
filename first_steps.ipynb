{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca9fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from tqdm.auto import tqdm\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from functools import partial\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jnp.arange(10)\n",
    "print(f\"a: {a}\")\n",
    "print(f\"a.shape: {a.shape}\")\n",
    "\n",
    "try:\n",
    "    # cursor tried to add this line (try/except added by me): \n",
    "    print(f\"a.device: {a.device()}\")\n",
    "except TypeError as x:\n",
    "    print(TypeError(x))\n",
    "    print(\"In jax, we generally don't have to worry about moving data between gpu and cpu!\")\n",
    "\n",
    "print(\"most vector/matrix operations are just like numpy/torch\")\n",
    "print(f\"a.T @ a: {a.T @ a}\")\n",
    "print(f\"a @ a: {a @ a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39be6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = jnp.zeros(10)\n",
    "print(f\"b: {b}\")\n",
    "\n",
    "try:\n",
    "    b[0] = 1\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "b = b.at[0].set(1)\n",
    "print(f\"b: {b}\")\n",
    "\n",
    "print(\"this can be a bit annoying, but restrictions like these allow for enourmous benefits (i.e. jit!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893fa730",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"But before jit, lets learn how rng works\")\n",
    "key = jr.PRNGKey(0)\n",
    "\n",
    "shape = (3,2)\n",
    "print(f\"random normal array:\\n{jr.normal(key, shape)}\")\n",
    "\n",
    "print(f\"lets do another one!\\n{jr.normal(key, shape)}\")\n",
    "\n",
    "print(f\"They're the same! This is because we used the same key, which is not a complicated object, it's just an array. The key: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"so when we want to generate some randomness, and have the next call be different, 'split' the key\")\n",
    "key, key2 = jr.split(key)\n",
    "print(f\"our new keys are:{key}, and {key2}\")\n",
    "random_array = jr.normal(key, shape)\n",
    "print(f\"a random array:\\n{random_array}\")\n",
    "key, key2 = jr.split(key)\n",
    "random_array = jr.normal(key, shape)\n",
    "print(f\"and another one!\\n{random_array}\")\n",
    "print(\"In general, its a good idea to explicitly keep track of rng keys, this helps with reproducibility, and to avoid subtle bugs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44371859",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Heres an implemtntation with {-1, 1}\")\n",
    "class ExampleSparseParity:\n",
    "    def __init__(self, n, k, key):\n",
    "        self.n = n\n",
    "        self.inds = jr.choice(key, n, shape=(k,), replace = False)\n",
    "\n",
    "    def get_example(self, key) -> tuple[jax.Array, jax.Array]:\n",
    "        X = 1 - 2 * jr.bernoulli(key, shape=(self.n,)).astype(jnp.float32)\n",
    "        y = jnp.prod(X[self.inds])\n",
    "        return X, y\n",
    "    \n",
    "    # @partial(jax.jit, static_argnums=(0,1))\n",
    "    @eqx.filter_jit\n",
    "    def get_batch(self, batch_size, key) -> tuple[jax.Array, jax.Array]:\n",
    "        X = 1 - 2 * jr.bernoulli(key, shape=(batch_size, self.n)).astype(jnp.float32)\n",
    "        y = jnp.prod(X[..., self.inds], axis=-1)\n",
    "        return X, y\n",
    "\n",
    "\n",
    "key = jr.PRNGKey(0)\n",
    "key, init_key = jr.split(key)\n",
    "test = ExampleSparseParity(4, 2, init_key)\n",
    "X, y = test.get_example(key)\n",
    "print(f\"X: {X}\")\n",
    "print(f\"y: {y}\")\n",
    "\n",
    "\n",
    "X, y = test.get_batch(2, key)\n",
    "print(f\"X: {X}\")\n",
    "print(f\"y: {y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df226d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Now you do it with {0, 1}!\")\n",
    "\n",
    "class SparseParity:\n",
    "    def __init__(self, n, k, key):\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "\n",
    "    def get_batch(self, batch_size, key) -> tuple[jax.Array, jax.Array]:\n",
    "        raise NotImplementedError(\"Not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453156af",
   "metadata": {},
   "source": [
    "Now lets train a model to learn the parity!\n",
    "To make things more friendly to those familiar with torch, we will use Equinox, a nn package for jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98264ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "key = jr.PRNGKey(0)\n",
    "key, dataset_key, model_init_key = jr.split(key, 3)\n",
    "n = 10\n",
    "k = 2\n",
    "\n",
    "dataset = SparseParity(n, k, dataset_key)\n",
    "\n",
    "hidden_dim = 32\n",
    "num_hidden_layers = 2\n",
    "\n",
    "\n",
    "# we'll use the builtin MLP from Equinox for now\n",
    "model = eqx.nn.MLP(in_size=n, out_size=1, width_size=hidden_dim, depth=num_hidden_layers, key=model_init_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434e980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = dataset.get_batch(10, key)\n",
    "try:\n",
    "    print(model(X))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"One quirk about equinox, is that it doesn't handle batching automatically, but this easy for us to fix with vmap\")\n",
    "print(\"when using equinox, sometimes you need to use filter versions of jax functions (for example: vmap, jit) to make things work, for now just always do so\")\n",
    "\n",
    "print(eqx.filter_vmap(model)(X))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb27c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "print(\"we use optax for optimizers\")\n",
    "optimizer = optax.adam(learning_rate=lr)\n",
    "\n",
    "# the core of the training loop is function that updates the model\n",
    "# its helpful that this is its own function, for reasons we will discuss later!\n",
    "def train_step(model, opt_state, batch):\n",
    "    X,y = batch\n",
    "    def loss(model):\n",
    "        y_preds = jax.vmap(model)(X).flatten()\n",
    "        return jnp.mean((y_preds - y)**2)\n",
    "\n",
    "    loss, grads = eqx.filter_value_and_grad(loss)(model)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, eqx.filter(model, eqx.is_array))\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss\n",
    "\n",
    "# write a function to record test metrics!\n",
    "def eval_step(model, batch):\n",
    "    raise NotImplementedError(\"Not implemented\")\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlp import MLP\n",
    "batch_size = 64\n",
    "num_iters = 200\n",
    "key = jr.PRNGKey(0)\n",
    "key, test_data_key, train_data_key, model_init_key = jr.split(key,4)\n",
    "test_batch = dataset.get_batch(100, test_data_key)\n",
    "\n",
    "\n",
    "dataset = SparseParity(n, k, dataset_key)\n",
    "\n",
    "hidden_dim = 32\n",
    "num_hidden_layers = 2\n",
    "\n",
    "model = eqx.nn.MLP(in_size=n, out_size=1, width_size=hidden_dim, depth=num_hidden_layers, key=model_init_key)\n",
    "\n",
    "def dataloader(key):\n",
    "    while True:\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "        yield batch\n",
    "\n",
    "train_loader = dataloader(train_data_key)\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "metrics = {\"train_loss\":[], \"test_loss\":[], \"test_accuracy\":[]}\n",
    "pbar = tqdm(range(num_iters))\n",
    "for i in pbar:\n",
    "    # update the model\n",
    "    raise NotImplementedError(\"Model update not implemented\")\n",
    "    \n",
    "    # record train loss and other metrics\n",
    "    raise NotImplementedError(\"Record metrics not implemented\")\n",
    "    pbar.set_postfix(test_loss=f\"{metrics['test_loss'][-1]:.3f}\", test_accuracy=f\"{metrics['test_accuracy'][-1]:.3f}\", train_loss=f\"{metrics['train_loss'][-1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0501fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics[\"train_loss\"], label=\"train loss\")\n",
    "plt.legend()\n",
    "plt.title(f\"Training Loss for MLP on Sparse Parity n={n}, k={k}\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "plt.plot(metrics[\"test_accuracy\"], label=\"test accuracy\")\n",
    "plt.plot(metrics[\"test_loss\"], label=\"test loss\") # might have to adjust if you don't take test loss every iteration\n",
    "plt.title(f\"Test Metrics for MLP on Sparse Parity n={n}, k={k}\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(eqx.Module):\n",
    "    layers: List[eqx.nn.Linear]\n",
    "\n",
    "    def __init__(self, in_size, out_size, width_size, depth, key):\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "\n",
    "    def __call__(self, x, key):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
