{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ca9fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from tqdm.auto import tqdm\n",
    "import optax\n",
    "from functools import partial\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9677edac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [0 1 2 3 4 5 6 7 8 9]\n",
      "a.shape: (10,)\n",
      "'jaxlib._jax.Device' object is not callable\n",
      "In jax, we generally don't have to worry about moving data between gpu and cpu!\n",
      "most vector/matrix operations are just like numpy/torch\n",
      "a.T @ a: 285\n",
      "a @ a: 285\n"
     ]
    }
   ],
   "source": [
    "a = jnp.arange(10)\n",
    "print(f\"a: {a}\")\n",
    "print(f\"a.shape: {a.shape}\")\n",
    "\n",
    "try:\n",
    "    # cursor tried to add this line (try/except added by me): \n",
    "    print(f\"a.device: {a.device()}\")\n",
    "except TypeError as x:\n",
    "    print(TypeError(x))\n",
    "    print(\"In jax, we generally don't have to worry about moving data between gpu and cpu!\")\n",
    "\n",
    "print(\"most vector/matrix operations are just like numpy/torch\")\n",
    "print(f\"a.T @ a: {a.T @ a}\")\n",
    "print(f\"a @ a: {a @ a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f39be6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "JAX arrays are immutable and do not support in-place item assignment. Instead of x[idx] = y, use x = x.at[idx].set(y) or another .at[] method: https://docs.jax.dev/en/latest/_autosummary/jax.numpy.ndarray.at.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "this can be a bit annoying, but restrictions like these allow for enourmous benefits (i.e. jit!)\n"
     ]
    }
   ],
   "source": [
    "b = jnp.zeros(10)\n",
    "print(f\"b: {b}\")\n",
    "\n",
    "try:\n",
    "    b[0] = 1\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "b = b.at[0].set(1)\n",
    "print(f\"b: {b}\")\n",
    "\n",
    "print(\"this can be a bit annoying, but restrictions like these allow for enourmous benefits (i.e. jit!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "893fa730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But before jit, lets learn how rng works\n",
      "random normal array:\n",
      "[[ 1.6226422   2.0252647 ]\n",
      " [-0.43359444 -0.07861735]\n",
      " [ 0.1760909  -0.97208923]]\n",
      "lets do another one!\n",
      "[[ 1.6226422   2.0252647 ]\n",
      " [-0.43359444 -0.07861735]\n",
      " [ 0.1760909  -0.97208923]]\n",
      "They're the same! This is because we used the same key, which is not a complicated object, it's just an array. The key: [0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"But before jit, lets learn how rng works\")\n",
    "key = jr.PRNGKey(0)\n",
    "\n",
    "shape = (3,2)\n",
    "print(f\"random normal array:\\n{jr.normal(key, shape)}\")\n",
    "\n",
    "print(f\"lets do another one!\\n{jr.normal(key, shape)}\")\n",
    "\n",
    "print(f\"They're the same! This is because we used the same key, which is not a complicated object, it's just an array. The key: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0749eee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so when we want to generate some randomness, and have the next call be different, 'split' the key\n",
      "our new keys are:[1797259609 2579123966], and [ 928981903 3453687069]\n",
      "a random array:\n",
      "[[ 1.0040143  -0.9063372 ]\n",
      " [-0.7481722  -1.1713669 ]\n",
      " [-0.8712328   0.58883816]]\n",
      "and another one!\n",
      "[[-0.57478017  0.79983664]\n",
      " [-0.25960687  1.429873  ]\n",
      " [-0.52380246 -1.7450135 ]]\n",
      "In general, its a good idea to explicitly keep track of rng keys, this helps with reproducibility, and to avoid subtle bugs\n"
     ]
    }
   ],
   "source": [
    "print(f\"so when we want to generate some randomness, and have the next call be different, 'split' the key\")\n",
    "key, key2 = jr.split(key)\n",
    "print(f\"our new keys are:{key}, and {key2}\")\n",
    "random_array = jr.normal(key, shape)\n",
    "print(f\"a random array:\\n{random_array}\")\n",
    "key, key2 = jr.split(key)\n",
    "random_array = jr.normal(key, shape)\n",
    "print(f\"and another one!\\n{random_array}\")\n",
    "print(\"In general, its a good idea to explicitly keep track of rng keys, this helps with reproducibility, and to avoid subtle bugs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44371859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heres an implemtntation with {0,1}\n",
      "X: [0. 1. 1. 1.]\n",
      "y: 1.0\n",
      "X: [[0. 1. 1. 1.]\n",
      " [1. 0. 0. 1.]]\n",
      "y: [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Heres an implemtntation with {0,1}\")\n",
    "class SparseParity:\n",
    "    def __init__(self, n, k, key):\n",
    "        self.n = n\n",
    "        self.inds = jr.choice(key, n, shape=(k,), replace = False)\n",
    "    \n",
    "    # @eqx.filter_jit\n",
    "    def get_batch(self, batch_size, key) -> tuple[jax.Array, jax.Array]:\n",
    "        X = jr.bernoulli(key, shape=(batch_size, self.n)).astype(jnp.float32)\n",
    "        y = jnp.sum(X[..., self.inds], axis=-1) % 2\n",
    "        return X, y\n",
    "\n",
    "\n",
    "key = jr.PRNGKey(0)\n",
    "key, init_key = jr.split(key)\n",
    "dataset = SparseParityExample(4, 2, init_key)\n",
    "X, y = dataset.get_example(key)\n",
    "print(f\"X: {X}\")\n",
    "print(f\"y: {y}\")\n",
    "\n",
    "\n",
    "X, y = dataset.get_batch(2, key)\n",
    "print(f\"X: {X}\")\n",
    "print(f\"y: {y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df226d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you do it with {1,-1} parity!\n",
    "\n",
    "class SparseParity:\n",
    "    def __init__(self, n, k, key):\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "\n",
    "    def get_example(self, key) -> tuple[jax.Array, jax.Array]:\n",
    "        raise NotImplementedError(\"Not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453156af",
   "metadata": {},
   "source": [
    "Now lets train a model to learn the parity!\n",
    "To make things more friendly to those familiar with torch, we will use Equinox, a nn package for jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f98264ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "key = jr.PRNGKey(0)\n",
    "key, dataset_key, model_init_key = jr.split(key, 3)\n",
    "n = 10\n",
    "k = 1\n",
    "\n",
    "dataset = SparseParity(n, k, dataset_key)\n",
    "\n",
    "hidden_dim = 32\n",
    "num_hidden_layers = 2\n",
    "\n",
    "model = eqx.nn.MLP(in_size=n, out_size=1, width_size=hidden_dim, depth=num_hidden_layers, key=model_init_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434e980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incompatible shapes for broadcasting: shapes=[(32, 10), (32,)]\n",
      "One quirk about equinox, is that it doesn't handle batching automatically, but this easy for us to fix with vmap\n",
      "when using equinox, sometimes you need to use filter versions of jax functions (for example: vmap, jit) to make things work, for now just always do so\n",
      "[[0.05863924]\n",
      " [0.08096123]\n",
      " [0.04649485]\n",
      " [0.06503659]\n",
      " [0.07639153]\n",
      " [0.06271922]\n",
      " [0.05976832]\n",
      " [0.04839244]\n",
      " [0.07761116]\n",
      " [0.05253061]]\n",
      "[0. 1. 1. 0. 0. 1. 0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "X,y = dataset.get_batch(10, key)\n",
    "try:\n",
    "    print(model(X))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"One quirk about equinox, is that it doesn't handle batching automatically, but this easy for us to fix with vmap\")\n",
    "print(\"when using equinox, sometimes you need to use filter versions of jax functions (for example: filter_vmap, filter_jit)\")\n",
    "\n",
    "print(eqx.filter_vmap(model)(X))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb27c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we use optax for optimizers\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "optimizer = optax.adam(learning_rate=lr)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def train_step(model, opt_state, batch):\n",
    "    X,y = batch\n",
    "    def loss(model):\n",
    "        y_preds = jax.vmap(model)(X).flatten()\n",
    "        return jnp.mean((y_preds - y)**2)\n",
    "\n",
    "    loss, grads = eqx.filter_value_and_grad(loss)(model)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, eqx.filter(model, eqx.is_array))\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss\n",
    "\n",
    "@eqx.filter_jit\n",
    "def eval_step(model, batch):\n",
    "    X,y = batch\n",
    "    y_preds = eqx.filter_vmap(model)(X).flatten()\n",
    "    loss = jnp.mean((y_preds - y)**2)\n",
    "\n",
    "    accuracy = jnp.mean((y_preds > .5) == (y > .5))\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce6f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368be1556e5441949ed6aad47ba9b58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from mlp import MLP\n",
    "batch_size = 64\n",
    "num_iters = 1000\n",
    "key = jr.PRNGKey(0)\n",
    "key, test_data_key, model_init_key = jr.split(key,3)\n",
    "test_batch = dataset.get_batch(64, test_data_key)\n",
    "key, datakey = jr.split(key)\n",
    "keys = jr.split(datakey, num_iters)\n",
    "batches = (dataset.get_batch(batch_size, key) for key in keys)\n",
    "dataset = SparseParity(n, k, dataset_key)\n",
    "\n",
    "hidden_dim = 16\n",
    "num_hidden_layers = 2\n",
    "\n",
    "model = eqx.nn.MLP(in_size=n, out_size=1, width_size=hidden_dim, depth=num_hidden_layers, key=model_init_key)\n",
    "def dataloader(key):\n",
    "    while True:\n",
    "        yield from batches\n",
    "        # key, datakey = jr.split(key)\n",
    "        # yield dataset.get_batch(batch_size, datakey)\n",
    "\n",
    "train_loader = dataloader(key)\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "pbar = tqdm(range(num_iters))\n",
    "for i in pbar:\n",
    "    model, opt_state, train_loss = train_step(model, opt_state, next(train_loader))\n",
    "    if i % 10 == 0:\n",
    "        test_loss, test_accuracy = eval_step(model, test_batch)\n",
    "        pbar.set_postfix(test_loss=f\"{test_loss:.3f}\", test_accuracy=f\"{test_accuracy:.3f}\", train_loss=f\"{train_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1193aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(eqx.Module):\n",
    "    layers: List[eqx.nn.Linear]\n",
    "\n",
    "    def __init__(self, in_size, out_size, width_size, depth, key):\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "\n",
    "    def __call__(self, x, key):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
